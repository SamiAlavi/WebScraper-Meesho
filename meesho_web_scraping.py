# -*- coding: utf-8 -*-
"""meesho web-scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11wTMxCxAJOnQATPQA07eYq7yBlicmnjf
"""

from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
from math import ceil
from time import time, sleep
from random import choice

def getRoutes():
  routes = list()
  main = 'https://meesho.com/ethnicwear-women/pl/5v80d?page=1'
  htmltext = urlopen(main).read()
  soup = BeautifulSoup(htmltext).select('a[role=tab]')
  for tag in soup:
    routes.append(tag['href'])
  return routes

def getMaxPage(main_url,all_routes):
  '''
    main_url : domain url
    all_routes : all tabs in the domain url
  '''
  all_pages = list()
  for route in all_routes:
    htmltext = urlopen(main_url+route).read()
    soup = BeautifulSoup(htmltext).select('div.plp-desc')[0].get_text().split(' ')
    pages = str(ceil(int(soup[-2])/20)) # total pages
    print(route,pages)
    all_pages.append(pages)
  return all_pages

def readProgress():
  try:
    with open('progress.txt','r',encoding='utf8') as f:
      index, start = f.read().split(',')
    print('\nGetting progress from progress.txt')
  except:
    print('\n======== Scraping from the start ========')
    index, start = 0, 1
  return int(index), int(start)

def readRoutes():
  try:
    with open('routes.txt','r',encoding='utf8') as f:
      all_routes = f.read().split(',')
    print('\nGetting routes from routes.txt')
  except:
    print('\n======== Scraping routes from urls ========')
    all_routes = getRoutes()
    writeRoutes(all_routes)
  return all_routes

def readPages(main_url,all_routes):
  try:
    with open('pages.txt','r',encoding='utf8') as f:
      all_pages = f.read().split(',')
    print('\nGetting pages from pages.txt')
    [print(all_routes[i],all_pages[i]) for i in range(len(all_routes))]
  except:
    print('\n======== Scraping page numbers from urls ========')
    all_pages = getMaxPage(main_url,all_routes)
    writePages(all_pages)
  all_pages = [int(i) for i in all_pages]
  return all_pages

def writeProgress(index,page):
  '''
    index : index of tab in all_tabs
    page : from which page of index tab to start scraping from
  '''
  with open('progress.txt','w',encoding='utf8') as f:
    f.write(f'{index},{page}')

def writeRoutes(all_routes):
  '''
    all_routes : all tabs of the domain
  '''
  with open('routes.txt','w',encoding='utf8') as f:
    f.write(','.join(all_routes))

def writePages(all_pages):
  '''
    all_pages : all pages of all tabs
  '''
  with open('pages.txt','w',encoding='utf8') as f:
    f.write(','.join(all_pages))

def writeCSV(fname, titles, prod_urls, images, costs, desc):
  '''
    fname : file name.csv
    titles : list of products' titles
    images : list of products' images
    costs : list of products' costs
    desc : list of products' descriptions
  '''
  sep1 = ';'  # for columns
  sep2 = '~'  # for description
  buffer = ''
  for i in range(len(titles)):
    description = sep2.join(desc[i])
    buffer+=f'{titles[i]}{sep1}{prod_urls[i]}{sep1}{images[i]}{sep1}{costs[i]}{sep1}{description}\n'      
  with open(fname+'.csv','a+',encoding='utf8') as f:
    f.write(buffer)

# mobile setup
#    
#      soup = BeautifulSoup(htmltext).select('a.plp-card')#z
#
#      # if page not found, break and go to next tab
#     if len(soup)==0:
#        break
#      # for each product (20 products per page)
#      for tag in soup:
#        next = tag.div.next_sibling
#        images.append(tag.img['data-src'])  # image
#        titles.append(next.get_text())  # title
#        costs.append(next.next_sibling.div.get_text()) # cost
#       prod_urls.append(main_url+tag['href'])
#        temp1 = getDescription(prod_urls[-1]) # description
#        desc.append(temp1)

def getDescription(prod_url):
  '''
    prod_url : product url/page
  '''
  desc = list()

  htmltext = urlopen(prod_url).read()
  soup = BeautifulSoup(htmltext).select('span.pre')

  # for each description line
  for tag in soup:
    desc.append(tag.get_text()+tag.next_sibling.get_text())
  return desc

def scrape(main_url, all_routes, all_pages, index, start, header):
  '''
    main_url : domain url
    all_routes : all tabs in the domain url
    all_pages : total pages for each tab
    index : which tab to start from
    start : which page of the tab to start from
  '''
  for index in range(index,len(all_routes)):
    route = all_routes[index]
    total = all_pages[index]+1
    fname = route.split('/')[1]
    print(f'{route} => {start}/{total}')

    # for each page
    #t1 = time()
    for i in range(start,total):
      images, titles, prod_urls  = list(),list(),list()
      costs, desc_header, desc = list(),list(),list()
      
      print(f'{main_url}{route}?page={i}')
      req = Request(f'{main_url}{route}?page={i}')
      req.add_header('Referer', main_url)
      htmltext = urlopen(req).read()
      soup = BeautifulSoup(htmltext).select('a')
      for tag in soup:
        print(tag)
      
      # if page not found, break and go to next tab
      if len(soup)==0:
        break
      # for each product (20 products per page)
      for tag in soup:
        next = tag.div.next_sibling
        images.append(tag.img['data-src'])  # image
        titles.append(next.get_text())  # title
        costs.append(next.next_sibling.div.get_text()) # cost
        prod_urls.append(main_url+tag['href'])
        temp1 = getDescription(prod_urls[-1]) # description
        desc.append(temp1)
      
      writeCSV(fname,titles,prod_urls,images,costs,desc)
      writeProgress(index,i+1)
      
      #if i%10==0:
      #  print(f'{i}/{total}\t{time()-t1}')
      #  t1 = time()
    start = 1

main_url = 'https://meesho.com'

user_agents = [
    'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
    'Opera/9.25 (Windows NT 5.1; U; en)',
    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
    'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',
    'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',
    'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9'
]

all_routes = readRoutes()
all_pages = readPages(main_url,all_routes)
index, start = readProgress()

header = { 'User-Agent' :  choice(user_agents)}
print(header)
print(f'Starting from {main_url+all_routes[index]}?page={start}')

scrape(main_url, all_routes, all_pages, index, start, header)